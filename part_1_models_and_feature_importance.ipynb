{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy import misc\n",
    "import collections\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(test, predict, model_name): \n",
    "    print('Accuracy Score of ' + model_name, accuracy_score(test, predict))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predict, pos_label=1)\n",
    "    print('AUC Score of ' + model_name, metrics.auc(fpr, tpr))\n",
    "    print(classification_report(test, predict))\n",
    "    \n",
    "def logistic_regression_model(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log = LogisticRegression()\n",
    "    \n",
    "    # Get the best parameter\n",
    "    params = {'penalty':['l1','l2'],\n",
    "              'solver': ['liblinear'],\n",
    "             'C':[0.01, 0.1, 1, 10, 100],\n",
    "             'class_weight':['balanced',None]}\n",
    "    model = GridSearchCV(log, param_grid=params, cv=10, iid=True)\n",
    "    \n",
    "    # result of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    print('The best parameter is ', model.best_params_)\n",
    "    model_report(predict, y_test, \"Logistic Regression\")\n",
    "    \n",
    "    \n",
    "def svc_model(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.svm import SVC\n",
    "    svc = SVC()\n",
    "    \n",
    "    # Get the best parameter\n",
    "    params = {'kernel': ['linear', 'rbf', 'sigmoid'], 'gamma': ['auto']}\n",
    "    model = GridSearchCV(svc, param_grid=params, cv=10, iid=True)\n",
    "    \n",
    "    # result of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('The best parameter is ', model.best_params_)\n",
    "    model_report(y_pred, y_test, \"SVM\")\n",
    "    \n",
    "    \n",
    "def decision_tree_model(X_train, X_test, y_train, y_test):\n",
    "    params = {\"max_depth\": [1, 2, 3, 4, None],\n",
    "              \"max_features\": [1, 2, 3, 4, None],\n",
    "              \"min_samples_leaf\": np.arange(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    tree = DecisionTreeClassifier()\n",
    "    model = GridSearchCV(tree, param_grid=params, cv=10, iid=True)\n",
    "    \n",
    "    # result of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    print('The best parameter is ', model.best_params_)\n",
    "    model_report(predict, y_test, \"Decision Tree\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_model():\n",
    "    data = pd.read_csv('heart_1.csv')\n",
    "    X = data.drop('target', axis = 1)\n",
    "    Y = data['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\n",
    "    print(\"********************Logistic Regression*****************************\")\n",
    "    logistic_regression_model(X_train, X_test, y_train, y_test)\n",
    "    print(\"********************SVM*****************************\")\n",
    "    svc_model(X_train, X_test, y_train, y_test)\n",
    "    print(\"********************Decision Tree*****************************\")\n",
    "    decision_tree_model(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_model_with_preprocessing():\n",
    "    data = pd.read_csv('heart_1.csv')\n",
    "    data['cp'] = data['cp'].astype('object')\n",
    "    data['slope'] = data['slope'].astype('object')\n",
    "    data['thal'] = data['thal'].astype('object')\n",
    "    data['restecg'] = data['restecg'].astype('object')\n",
    "    data = pd.get_dummies(data)\n",
    "    X = data.drop('target', axis = 1)\n",
    "    Y = data['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\n",
    "    # Use StandardScaler to preprocess\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standard_scaler = StandardScaler()\n",
    "\n",
    "    X_train_2 = standard_scaler.fit_transform(X_train)\n",
    "    \n",
    "    X_train_2 = pd.DataFrame(X_train_2)\n",
    "\n",
    "    X_test_2 = standard_scaler.transform(X_test)\n",
    "    X_test_2 = pd.DataFrame(X_test_2)\n",
    "    print(\"********************Logistic Regression*****************************\")\n",
    "    logistic_regression_model(X_train_2, X_test_2, y_train, y_test)\n",
    "    print(\"********************SVM*****************************\")\n",
    "    svc_model(X_train_2, X_test_2, y_train, y_test)\n",
    "    print(\"********************Decision Tree*****************************\")\n",
    "    decision_tree_model(X_train_2, X_test_2, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Logistic Regression*****************************\n",
      "The best parameter is  {'C': 1, 'class_weight': None, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Accuracy Score of Logistic Regression 0.8131868131868132\n",
      "AUC Score of Logistic Regression 0.811764705882353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79        40\n",
      "           1       0.84      0.82      0.83        51\n",
      "\n",
      "    accuracy                           0.81        91\n",
      "   macro avg       0.81      0.81      0.81        91\n",
      "weighted avg       0.81      0.81      0.81        91\n",
      "\n",
      "********************SVM*****************************\n",
      "The best parameter is  {'gamma': 'auto', 'kernel': 'linear'}\n",
      "Accuracy Score of SVM 0.8131868131868132\n",
      "AUC Score of SVM 0.811764705882353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79        40\n",
      "           1       0.84      0.82      0.83        51\n",
      "\n",
      "    accuracy                           0.81        91\n",
      "   macro avg       0.81      0.81      0.81        91\n",
      "weighted avg       0.81      0.81      0.81        91\n",
      "\n",
      "********************Decision Tree*****************************\n",
      "The best parameter is  {'criterion': 'entropy', 'max_depth': None, 'max_features': 4, 'min_samples_leaf': 4}\n",
      "Accuracy Score of Decision Tree 0.6703296703296703\n",
      "AUC Score of Decision Tree 0.6834657398212511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.60      0.68        53\n",
      "           1       0.58      0.76      0.66        38\n",
      "\n",
      "    accuracy                           0.67        91\n",
      "   macro avg       0.68      0.68      0.67        91\n",
      "weighted avg       0.70      0.67      0.67        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Logistic Regression*****************************\n",
      "The best parameter is  {'C': 0.01, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Accuracy Score of Logistic Regression 0.8571428571428571\n",
      "AUC Score of Logistic Regression 0.8558994197292069\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85        44\n",
      "           1       0.84      0.89      0.87        47\n",
      "\n",
      "    accuracy                           0.86        91\n",
      "   macro avg       0.86      0.86      0.86        91\n",
      "weighted avg       0.86      0.86      0.86        91\n",
      "\n",
      "********************SVM*****************************\n",
      "The best parameter is  {'gamma': 'auto', 'kernel': 'sigmoid'}\n",
      "Accuracy Score of SVM 0.8131868131868132\n",
      "AUC Score of SVM 0.8112244897959184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80        42\n",
      "           1       0.82      0.84      0.83        49\n",
      "\n",
      "    accuracy                           0.81        91\n",
      "   macro avg       0.81      0.81      0.81        91\n",
      "weighted avg       0.81      0.81      0.81        91\n",
      "\n",
      "********************Decision Tree*****************************\n",
      "The best parameter is  {'criterion': 'entropy', 'max_depth': 4, 'max_features': None, 'min_samples_leaf': 6}\n",
      "Accuracy Score of Decision Tree 0.6373626373626373\n",
      "AUC Score of Decision Tree 0.6379227053140096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.59      0.62        46\n",
      "           1       0.62      0.69      0.65        45\n",
      "\n",
      "    accuracy                           0.64        91\n",
      "   macro avg       0.64      0.64      0.64        91\n",
      "weighted avg       0.64      0.64      0.64        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all_model_with_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importance():\n",
    "    # Calcuate feature importance using logistic regression model\n",
    "    data = pd.read_csv('heart_1.csv')\n",
    "    data['cp'] = data['cp'].astype('object')\n",
    "    data['slope'] = data['slope'].astype('object')\n",
    "    data['thal'] = data['thal'].astype('object')\n",
    "    data['restecg'] = data['restecg'].astype('object')\n",
    "    data = pd.get_dummies(data)\n",
    "    print(data.columns)\n",
    "    X = data.drop('target', axis = 1)\n",
    "    Y = data['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\n",
    "    # Use StandardScaler to preprocess\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = standard_scaler.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "\n",
    "    X_test = standard_scaler.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    #use best parameters we get from previous steps\n",
    "    best_log = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=0.01,\n",
    "        solver='liblinear',\n",
    "        class_weight=None,\n",
    "    )\n",
    "    best_log.fit(X_train, y_train)\n",
    "    feature_importance = abs(best_log.coef_[0])\n",
    "    sorted_index = np.argsort(feature_importance)\n",
    "    sorted_features = []\n",
    "    for i in sorted_index:\n",
    "        sorted_features.append(X.columns[i])\n",
    "    print(\"Sort features ascendingly based on feauture importance: \")\n",
    "    print(sorted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'ca', 'target', 'cp_0', 'cp_1', 'cp_2', 'cp_3', 'restecg_0',\n",
      "       'restecg_1', 'restecg_2', 'slope_0', 'slope_1', 'slope_2', 'thal_0',\n",
      "       'thal_1', 'thal_2', 'thal_3'],\n",
      "      dtype='object')\n",
      "Sort features ascendingly based on feauture importance: \n",
      "['thal_1', 'slope_0', 'trestbps', 'chol', 'thal_0', 'fbs', 'cp_1', 'restecg_0', 'restecg_2', 'age', 'cp_3', 'restecg_1', 'slope_1', 'slope_2', 'thalach', 'sex', 'cp_2', 'oldpeak', 'exang', 'cp_0', 'thal_2', 'thal_3', 'ca']\n"
     ]
    }
   ],
   "source": [
    "calculate_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
